{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "8ae586d5-6c80-4661-9267-120b5c939cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "287c7003-f031-45ec-b4d6-8419567a3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Small_Block_bn(nn.Module):\n",
    "    def __init__(self,r,k,in_ch,drop_rate):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.conv1=nn.Conv1d(int(in_ch),int(in_ch/r/k),1,stride=1)\n",
    "        self.bn1=nn.BatchNorm1d(int(in_ch/r/k))\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.conv2=nn.Conv1d(int(in_ch/r/k),int(in_ch/k),3,stride=1,padding=1)\n",
    "        self.bn2=nn.BatchNorm1d(int(in_ch/k))\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        #print(\"The shape of data aftr 1st conv should be [1,c'/r/k,w] :{}\".format(x.shape))\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.drop(x)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=self.bn2(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.drop(x)\n",
    "        #print(\"The shape of data aftr 2nd conv should be [1,c'/k,w] :{}\".format(x.shape))\n",
    "        #print(\"#############################################\")\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "047b40f9-adbe-4e86-bcbe-78049e4d025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Small_Block(nn.Module):\n",
    "    def __init__(self,r,k,in_ch,drop_rate):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.conv1=nn.Conv1d(int(in_ch),int(in_ch/r/k),1,stride=1)\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.conv2=nn.Conv1d(int(in_ch/r/k),int(in_ch/k),3,stride=1,padding=1)\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.drop(x)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.drop(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "3b07c61e-639f-4fa1-88c2-de5e99122e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cardinal(nn.Module):\n",
    "    def __init__(self, r, k, in_ch, drop, up, radix=Small_Block_bn, radixUp=Small_Block):\n",
    "        super().__init__()\n",
    "        if up==False:\n",
    "            self.__radix_block=radixUp #small block\n",
    "        else:\n",
    "            self.__radix_block=radix #small block\n",
    "        self.k=k #number of cardinals  \n",
    "        self.r=r #number of radixes \n",
    "        self.in_ch=in_ch\n",
    "        self.drop_rate=drop\n",
    "        self.radix=self.__build_radix() #sends to build layers\n",
    "       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #concat tensors for every k radixs'\n",
    "        radix_current=[]\n",
    "        for i, l in enumerate(self.radix):\n",
    "            radix_current.append(l(x))\n",
    "        radix_concat = torch.cat(radix_current, dim=1)\n",
    "        \n",
    "        return radix_concat\n",
    "            \n",
    "    def __build_radix(self):\n",
    "        rdx=[]\n",
    "      \n",
    "        for i in range(self.k):\n",
    "            rdx.append(\n",
    "            self.__radix_block(self.r,self.k,self.in_ch,self.drop_rate)\n",
    "            )\n",
    "        out = nn.ModuleList(rdx)\n",
    "            \n",
    "        return nn.Sequential(*out)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "d07f7b56-d363-4cf1-9826-9ae0cb50e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeSt(nn.Module):\n",
    "    def __init__(self, r, k, stride,drop_rate, input_channels, ch_out, up, cardinal_block=Cardinal):\n",
    "        super().__init__()\n",
    "        self.cardinal_block=cardinal_block\n",
    "        #self.split=__build_split(input_channels)\n",
    "        self.in_ch=input_channels\n",
    "        self.drop_rate=drop_rate\n",
    "        \n",
    "        self.r=r\n",
    "        self.k=k\n",
    "        self.up=up\n",
    "        self.pool=nn.MaxPool1d(stride=2,kernel_size=2)\n",
    "        self.pool_non=nn.MaxPool1d(stride=1,kernel_size=1)\n",
    "        self.upsample=nn.ConvTranspose1d(ch_out, ch_out, kernel_size=2, stride=2)\n",
    "       \n",
    "        self.conv1=nn.Conv1d(\n",
    "        in_channels=input_channels,\n",
    "        out_channels=input_channels,\n",
    "        kernel_size=1,\n",
    "        )  \n",
    "        self.conv2=nn.Conv1d(\n",
    "        in_channels=input_channels,\n",
    "        out_channels=ch_out,\n",
    "        kernel_size=1,\n",
    "        )\n",
    "        \n",
    "        self.cardinal=self.build_cardinal_block(up)\n",
    "        \n",
    "        if up == True:\n",
    "            self.resenst_block = nn.Sequential(*[self.cardinal,self.conv2,self.pool])\n",
    "        elif up == False:\n",
    "            self.resenst_block = nn.Sequential(*[self.cardinal, self.conv2, self.upsample])\n",
    "        else: #None\n",
    "            self.resenst_block = nn.Sequential(*[self.cardinal, self.conv2, self.pool_non])\n",
    "\n",
    "            \n",
    "        self.global_pool=nn.AdaptiveAvgPool1d(int(1))\n",
    "        self.dence1=nn.Linear(int(input_channels),int(input_channels/2))\n",
    "        self.dence2=nn.Linear(int(input_channels/2),int(input_channels))\n",
    "        self.relu=nn.ReLU()\n",
    "        self.drop=nn.Dropout(0.1)\n",
    "        self.softmax=nn.Softmax(dim=2)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        transformant=0\n",
    "        #add every r cardinal\n",
    "        residual=[]\n",
    "        for i in range(self.r):\n",
    "            r_block=self.resenst_block[0][i](x)\n",
    "            transformant=transformant+r_block\n",
    "            #print(transformant.shape)\n",
    "            residual.append(r_block)\n",
    "        \n",
    "###############stuff here \n",
    "\n",
    "        vector=self.global_pool(transformant).mean(2) #shapes [1,8,1] => [1,8]\n",
    "        branch=self.dence1(vector) \n",
    "        branch=self.relu(branch)\n",
    "        branch=self.drop(branch)\n",
    "        ##need batch norm\n",
    "        dout=[]\n",
    "        branch.shape\n",
    "        for t in range(self.r):\n",
    "            variable=self.dence2(branch)\n",
    "            variable=self.relu(variable)\n",
    "            variable=self.drop(variable)\n",
    "            variable=variable.unsqueeze(dim=2)\n",
    "            dout.append(variable)\n",
    "        variable = torch.cat(dout, dim=0)#<2  #torch.Size([2, 4])\n",
    "\n",
    "        branch=self.softmax(variable)\n",
    "        #branch=branch.unsqueeze(dim=2) #shapes [1,8] => [1,8,1]\n",
    "        \n",
    "        trans=0\n",
    "        for t in range(self.r):\n",
    "            trans=trans+residual[t]*branch[t].unsqueeze(dim=0)  #shapes [1,8,3000] * [1,8,1]\n",
    "            \n",
    "        #torch.Size([1, 4...16,32,64])\n",
    "        trans=self.conv1(trans)\n",
    "        #print(\"shape of transformant {}, shape of branch {}\".format(transformant.shape,x.shape))\n",
    "        trans=trans+x #adding from x\n",
    "        trans=self.resenst_block[1](trans)    \n",
    "        trans=self.resenst_block[2](trans)    \n",
    "        print(trans.shape)\n",
    "        return trans\n",
    "        \n",
    "    \n",
    "    def build_cardinal_block(self,up):\n",
    "        crb=[]\n",
    "        \n",
    "        for i in range (self.r):\n",
    "            crb.append(\n",
    "            self.cardinal_block(self.r,self.k,self.in_ch,self.drop_rate,up)\n",
    "            )\n",
    "        out = nn.ModuleList(crb)\n",
    "        return nn.Sequential(*out)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "171417b0-e679-4c40-b0f1-b97d495d7e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4432, -0.2075, -0.9288],\n",
      "        [-0.7698,  0.6224, -0.5034]])\n",
      "tensor([[0.5633, 0.2939, 0.1428],\n",
      "        [0.1580, 0.6358, 0.2062]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "f8d7c3c0-9d64-4546-b7f7-623ef1cae284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,hparams, resnest=ResNeSt):\n",
    "        super().__init__()\n",
    "        self.resnest_block=resnest\n",
    "        self.hparams=hparams\n",
    "        self.r=hparams['r_k'][0]\n",
    "        self.k=hparams['r_k'][1]\n",
    "        self.stride=hparams[\"stride\"]\n",
    "        self.drop_rate=self.hparams['dropout_rate']\n",
    "        self.encoder=self.build_encoder()\n",
    "        self.decoder=self.build_decoder()\n",
    "        shape_out = self.hparams['n_samples']\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.hparams['layer_feature_maps'])-1):\n",
    "            shape_out //= self.hparams['pool_size']\n",
    "\n",
    "        for i in range(len(self.hparams['layer_feature_maps'])-1):\n",
    "            shape_out *= self.hparams['pool_size']\n",
    "        \n",
    "        self.__out_cnn = nn.Conv1d(\n",
    "            in_channels=self.hparams[\"layer_feature_maps\"][0],\n",
    "            out_channels=self.hparams[\"n_channels\"],\n",
    "            kernel_size=self.hparams[\"kernel_size\"],\n",
    "            padding=(self.hparams['n_samples'] - shape_out) // 2,\n",
    "            dilation=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        x=self.__out_cnn(x)\n",
    "        return x  \n",
    "        \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        resnest=[]\n",
    "        resnest.append(self.resnest_block(self.r,self.k, self.stride, self.drop_rate, self.hparams[\"n_channels\"],self.hparams[\"layer_feature_maps\"][0],None))\n",
    "        for i in range(len(self.hparams[\"layer_feature_maps\"])-1):\n",
    "            resnest.append(self.resnest_block(self.r,self.k,self.stride,self.drop_rate, self.hparams[\"layer_feature_maps\"][i],self.hparams[\"layer_feature_maps\"][i+1],True))\n",
    "        \n",
    "        return nn.Sequential(*resnest)  \n",
    "    \n",
    "        \n",
    "    def build_decoder(self):\n",
    "        resnest=[]\n",
    "\n",
    "        lfm=self.hparams['layer_feature_maps']\n",
    "        lfm.reverse()\n",
    "\n",
    "        resnest.append(self.resnest_block(self.r, self.k,self.stride,self.drop_rate, self.hparams[\"layer_feature_maps\"][0],self.hparams[\"layer_feature_maps\"][0],None))\n",
    "        \n",
    "        for i in range(len(self.hparams[\"layer_feature_maps\"])-1):\n",
    "            resnest.append(self.resnest_block(self.r, self.k,self.stride,self.drop_rate, self.hparams[\"layer_feature_maps\"][i],self.hparams[\"layer_feature_maps\"][i+1],False))\n",
    "        \n",
    "        lfm.reverse()\n",
    "        return nn.Sequential(*resnest) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "b5bcc466-ca9c-4750-80ac-6f5f916ccc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3000])\n",
      "torch.Size([1, 32, 1500])\n",
      "torch.Size([1, 64, 750])\n",
      "torch.Size([1, 128, 375])\n",
      "torch.Size([1, 256, 187])\n",
      "torch.Size([1, 256, 187])\n",
      "torch.Size([1, 128, 374])\n",
      "torch.Size([1, 64, 748])\n",
      "torch.Size([1, 32, 1496])\n",
      "torch.Size([1, 16, 2992])\n"
     ]
    }
   ],
   "source": [
    "hparams={\"kernel_size\":1,\"pool_size\":2,\"r_k\":[2,2],\"n_channels\":4,\"layer_feature_maps\":[16,32,64,128,256],\"dropout_rate\":0.1,'n_samples':3000,\"stride\":2}\n",
    "#n_channels=c_prime                            #3,2               30\n",
    "num_ch=hparams[\"n_channels\"]; \n",
    "num_samp=hparams[\"n_samples\"]\n",
    "inp=10+np.random.randint(0,20,(num_ch,num_samp))/5\n",
    "x=torch.tensor([inp]) \n",
    "x = x.type(torch.float)\n",
    "\n",
    "#net=ResNeSt(hparams,30,20)\n",
    "#out=net.forward(x) #torch.Size([1, 30, 300])\n",
    "\n",
    "net=AutoEncoder(hparams)\n",
    "out=net.forward(x) #torch.Size([1, 30, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "id": "264e915e-f1c2-438b-abf7-4812c2aa0f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3000])"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "6d096218-3532-4350-946b-26fb8ffddd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "tensor([[[ 0.5327, -0.8996, -0.0028],\n",
      "         [-1.1559, -0.0129, -1.7405]]])\n",
      "torch.Size([1, 2, 1])\n",
      "tensor([[[0.8170],\n",
      "         [1.3763]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3518e-01, -7.3494e-01, -2.2482e-03],\n",
       "         [-1.5908e+00, -1.7687e-02, -2.3955e+00]]])"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.AdaptiveAvgPool1d(1)\n",
    "input1 = torch.randn(1, 2, 3)\n",
    "print(input1.shape)\n",
    "print(input1)\n",
    "branch=torch.randn(1, 2)\n",
    "branch=branch.unsqueeze(dim=2)\n",
    "print(branch.shape)\n",
    "print(branch)\n",
    "input1*branch\n",
    "#input1=input1.squeeze(dim=[1,1])\n",
    "#input1=input1.mean(2)\n",
    "#print(input1.shape)\n",
    "#print(input1[0])\n",
    "#output = m(input1)\n",
    "\n",
    "#print(output.shape)\n",
    "#print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
