{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8baa8da4-d653-45b9-8efa-668091ab23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a3ca6bba-778a-4e8e-a5e7-b616e811046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBRD_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, drop_rate, pool_size, dilation):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv=nn.Conv1d(\n",
    "        in_ch,\n",
    "        in_ch,\n",
    "        kernel_size,\n",
    "        padding=int((kernel_size +(kernel_size -1) * (dilation-1)) /2),\n",
    "        dilation=dilation,\n",
    "        stride=1,\n",
    "        bias=False,\n",
    "        )\n",
    "        \n",
    "        self.bn=nn.BatchNorm1d(out_ch)\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_ch,\n",
    "            in_ch,\n",
    "            kernel_size,\n",
    "            padding=int(  (kernel_size+(kernel_size-1)*(dilation-1)) /2  ),\n",
    "            dilation=dilation,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_ch,\n",
    "            out_ch,\n",
    "            kernel_size,\n",
    "            padding=int(  (kernel_size+(kernel_size-1)*(dilation-1)) /2  ),\n",
    "            dilation=dilation,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        self.pooling=nn.MaxPool1d(kernel_size=pool_size)\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.conv(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        x=self.conv3(x)\n",
    "        x=self.bn(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        x=self.drop(x)\n",
    "        x=self.pooling(x)\n",
    "        return(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d350f7dc-d79b-4561-9096-7e173226c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsamp_arch=CBRD_layer(1,5,3,0.1,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c6f6ead7-6f6b-4509-9aa9-81af870912da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20])\n",
      "torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "#For Convolution \n",
    "x= torch.tensor([[[1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0]]])\n",
    "print(x.shape)\n",
    "p = x.type(torch.float)\n",
    "out_conv=upsamp_arch.conv(p)\n",
    "print(out_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0fbb2364-aba0-422b-a307-583cf24535a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20])\n",
      "torch.Size([1, 1, 20])\n",
      "torch.Size([1, 1, 20])\n",
      "torch.Size([1, 1, 20])\n",
      "torch.Size([1, 5, 20])\n",
      "torch.Size([1, 5, 20])\n",
      "torch.Size([1, 5, 20])\n",
      "torch.Size([1, 5, 20])\n",
      "torch.Size([1, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "#For enchoder archetecture \n",
    "x= torch.tensor([[[1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0]]])\n",
    "p = x.type(torch.float)\n",
    "p1=upsamp_arch.conv(p)\n",
    "print(p1.shape)\n",
    "p2=upsamp_arch.relu(p1)\n",
    "print(p2.shape)\n",
    "p3=upsamp_arch.conv2(p2)\n",
    "print(p3.shape)\n",
    "p4=upsamp_arch.relu(p3)\n",
    "print(p4.shape)\n",
    "p5=upsamp_arch.conv3(p4)\n",
    "print(p5.shape)\n",
    "p6=upsamp_arch.bn(p5)\n",
    "print(p6.shape)\n",
    "p7=upsamp_arch.relu(p6)\n",
    "print(p7.shape)\n",
    "p8=upsamp_arch.drop(p7)\n",
    "print(p8.shape)\n",
    "p9=upsamp_arch.pooling(p8)\n",
    "print(p9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1c704f12-c9e2-4046-972f-033d5932b834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsamp_arch.forward(p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4f467e98-d5bb-4a5b-954d-7d8ee48e117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRD_layer_upsample(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, drop_rate, scale_factor, dilation):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv1d(\n",
    "        in_ch,\n",
    "        out_ch,\n",
    "        kernel_size,\n",
    "        padding=int((kernel_size+(kernel_size-1)*(dilation-1))/2),\n",
    "        dilation=dilation,\n",
    "        stride=1,\n",
    "        bias=False  #why bias false?\n",
    "        )\n",
    "        self.relu=nn.ReLU()\n",
    "        self.upsample=nn.Upsample(scale_factor=scale_factor, mode='linear', align_corners=True)\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.conv2=nn.Conv1d(\n",
    "        out_ch,\n",
    "        out_ch,\n",
    "        kernel_size,\n",
    "        padding=int((kernel_size+(kernel_size-1)*(dilation-1))/2),\n",
    "        dilation=dilation,\n",
    "        stride=1,\n",
    "        bias=False,\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.upsample(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        x=self.conv(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        x=self.drop(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dbc320fe-f719-4b35-9139-28428982073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20])\n",
      "torch.Size([1, 1, 40])\n"
     ]
    }
   ],
   "source": [
    "#Decorer Layer explore\n",
    "x= torch.tensor([[[1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0]]])\n",
    "print(x.shape)\n",
    "\n",
    "upsamp=CRD_layer_upsample(1,1,5,0.1,2,1)\n",
    "y=upsamp.forward(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "715459d8-fadc-4e00-ab7d-9006028ec5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG:\n",
    "    def __init__(self,hparams, encoder_block=CBRD_layer, decoder_block=CRD_layer_upsample):\n",
    "        super().__init__()\n",
    "        \n",
    "        dilation=1\n",
    "        \n",
    "        self.encoder_block=encoder_block\n",
    "        self.decoder_block=decoder_block\n",
    "        self.hparams=hparams\n",
    "        \n",
    "        self.encoder=self.build_encoder() #this function looks hard\n",
    "        self.decoder=self.build_decoder()   #this function looks hard\n",
    "        \n",
    "        shape_out=self.hparams['n_samples']\n",
    "        \n",
    "        for i in range(len(self.hparams['layer_feature_maps'])):\n",
    "            shape_out=shape_out/self.hparams['pool_size']\n",
    "            \n",
    "        for i in range(len(self.hparams['layer_feature_maps'])):\n",
    "            shap_out=shape_out*self.hparams['pool_size']  #is it for final shape?\n",
    "            \n",
    "        \n",
    "        self.out_cnn=nn.Conv1d(\n",
    "        in_channels=self.hparams['n_channels'],\n",
    "        out_channels=self.hparams['n_channels'],\n",
    "        kernel_size=self.hparams['kernel_size'],\n",
    "        padding =int((self.hparams['n_samples'] - shape_out) // 2 + 1), #double slash float=> int\n",
    "        dilation=dilation,\n",
    "        stride=1,\n",
    "        bias=False,\n",
    "        ) \n",
    "        \n",
    "        self.sview=self.sview()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x=x.permute(0,2,1)  #changes shape of input from torch.Size([1, 1, 20]) to torch.Size([1, 20, 1])\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        x=self.out_cnn(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def sview(self):\n",
    "        network=[]\n",
    "        network.append(self.encoder)\n",
    "        network.append(self.decoder)\n",
    "        network.append(self.out_cnn)\n",
    "        return(nn.Sequential(*network))\n",
    "    \n",
    "\n",
    "    def get_embeddings(self,x):\n",
    "        x=x.permute(0,2,1)\n",
    "        x=self.enchoder(x)\n",
    "        x=torch.mean(x,dim=1)\n",
    "        return(x)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        layer_feature_maps=self.hparams['layer_feature_maps'].copy() #16 32 64 128 256 512\n",
    "        encoder=[]\n",
    "\n",
    "        for index, layer in enumerate(layer_feature_maps): #0,1,2,3,4,5  #16,32,64,128,256,512\n",
    "            if index==0: #for the first layer            \n",
    "                encoder.append(\n",
    "\n",
    "                    # f\"encoder_layer_{index}\",\n",
    "                    self.encoder_block(\n",
    "                    in_ch=self.hparams[\"n_channels\"],\n",
    "                    out_ch=layer_feature_maps[index],\n",
    "                    kernel_size=self.hparams[\"kernel_size\"],\n",
    "                    drop_rate=self.hparams[\"dropout_rate\"],\n",
    "                    pool_size=self.hparams[\"pool_size\"],\n",
    "                    dilation=self.hparams[\"dilation\"],\n",
    "                    ),\n",
    "            )\n",
    "            else:\n",
    "                encoder.append(\n",
    "                self.encoder_block(\n",
    "                in_ch=layer_feature_maps[index - 1],\n",
    "                out_ch=layer_feature_maps[index],\n",
    "                kernel_size=self.hparams[\"kernel_size\"],\n",
    "                drop_rate=self.hparams[\"dropout_rate\"],\n",
    "                pool_size=self.hparams[\"pool_size\"],\n",
    "                dilation=self.hparams[\"dilation\"],\n",
    "                ),\n",
    "                )\n",
    "        return(nn.Sequential(*encoder))\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \n",
    "        layer_feature_maps=self.hparams['layer_feature_maps'].copy()\n",
    "        layer_feature_maps.reverse() #512 256 128 64 32 16\n",
    "        \n",
    "        decoder=[]\n",
    "        \n",
    "        for index, layer in enumerate(layer_feature_maps):\n",
    "            if index == len(layer_feature_maps) - 1: #5 or like the last layer \n",
    "                decoder.append(\n",
    "                self.decoder_block(\n",
    "                in_ch=layer_feature_maps[index],\n",
    "                out_ch=self.hparams[\"n_channels\"],\n",
    "                kernel_size=self.hparams[\"kernel_size\"],\n",
    "                drop_rate=self.hparams[\"dropout_rate\"],\n",
    "                scale_factor=self.hparams[\"pool_size\"],\n",
    "                dilation=self.hparams[\"dilation\"],\n",
    "                )\n",
    "                )\n",
    "            else:\n",
    "                decoder.append(\n",
    "                 self.decoder_block(\n",
    "                in_ch=layer_feature_maps[index],\n",
    "                out_ch=layer_feature_maps[index + 1],\n",
    "                kernel_size=self.hparams[\"kernel_size\"],\n",
    "                drop_rate=self.hparams[\"dropout_rate\"],\n",
    "                scale_factor=self.hparams[\"pool_size\"],\n",
    "                dilation=self.hparams[\"dilation\"],\n",
    "                    ),\n",
    "                )\n",
    "        return nn.Sequential(*decoder)\n",
    "                \n",
    "\n",
    "                        \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c3b36acb-4e67-4cfe-9367-804b89d0cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams={\"pool_size\":2,\"layer_feature_maps\":[16,32,64,128,256,512],\"n_channels\":1,\"kernel_size\":3,\"dropout_rate\":0.1,\"pool_size\":2,\"dilation\":1,'n_samples':300}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6178ef51-e9dc-40c0-b70b-dda628711de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 300])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp=10+np.random.randint(0,20,300)/5\n",
    "x=torch.tensor([[inp]])\n",
    "x = x.type(torch.float)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "0ff35b9b-be41-4a49-b716-e2a53f98d3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 550])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsamp=VGG(hparams)\n",
    "y=upsamp.forward(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "07c70eba-7cc8-4cef-9ab4-40f1dbd68225",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=upsamp.sview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "1192759f-11ad-4cbc-a396-67cb2ffa4d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBRD_layer(\n",
       "  (conv): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (conv2): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "  (conv3): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "  (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b5d6aaa3-2c64-44e8-b391-3f664809516c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): CBRD_layer(\n",
       "      (conv): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (conv3): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): CBRD_layer(\n",
       "      (conv): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (conv3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): CBRD_layer(\n",
       "      (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): CBRD_layer(\n",
       "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): CBRD_layer(\n",
       "      (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): CBRD_layer(\n",
       "      (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (conv3): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): CRD_layer_upsample(\n",
       "      (conv): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (relu): ReLU()\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=linear)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    )\n",
       "    (1): CRD_layer_upsample(\n",
       "      (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (relu): ReLU()\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=linear)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    )\n",
       "    (2): CRD_layer_upsample(\n",
       "      (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (relu): ReLU()\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=linear)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    )\n",
       "    (3): CRD_layer_upsample(\n",
       "      (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (relu): ReLU()\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=linear)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    )\n",
       "    (4): CRD_layer_upsample(\n",
       "      (conv): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (relu): ReLU()\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=linear)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    )\n",
       "    (5): CRD_layer_upsample(\n",
       "      (conv): Conv1d(16, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (relu): ReLU()\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=linear)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (conv2): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (2): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(148,), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f60711-114e-42b8-bee5-50d0ce73b452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "571e8926-deba-486b-a3e6-2d4855f51a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Garbage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a165c3e3-9950-454c-8624-77d445d5cc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20])\n",
      "torch.Size([1, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "x= torch.tensor([[[1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0,1.0,2.0,3.0,4.0,5.0]]])\n",
    "print(x.shape)\n",
    "x=x.permute(0,2,1)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "64b6c8b0-c133-4528-ab7f-c8ed8e2eaa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "torch.Size([3, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "#This is what permute is doing \n",
    "aten = torch.tensor([[1, 2, 3], [4, 5, 6], [7,8, 9]])\n",
    "print(aten)\n",
    "print(aten.shape)\n",
    "# swapping the axes/dimensions 0 and 1\n",
    "aten=aten.permute(0, 1)\n",
    "print(aten)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
